# -*- coding: utf-8 -*-
"""Tokenization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15w0PsfBuZ0hynH_fMAduMrMlIbpyjWgE

#**split**
"""

text = """Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method."""
text.split(". ")

text.split(" ")

"""#**regex**"""

import re
tokens_sent = re.compile('[.!?] ').split(text) # Using compile method to combine RegEx patterns
tokens_sent

import re

text = """There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling."""
tokens = re.findall("[\w]+", text)
print(tokens)

"""# **NLTK**"""

!pip install nltk
import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize , sent_tokenize
text = """There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling."""
word_tokenize(text)

sent_tokenize(text)

"""#**spaCy**"""

!pip install spacy
!python -m spacy download en

from spacy.lang.en import English
nlp = English()
my_doc = nlp(text)
token_list = []
for token in my_doc:
    token_list.append(token.text)

print(token_list)

nlp = English()
nlp.add_pipe('sentencizer')
doc = nlp(text)
sentence_list =[]
for sentence in doc.sents:
    sentence_list.append(sentence.text)
print(sentence_list)

""" #Keras"""

from keras.preprocessing.text import text_to_word_sequence
text = """There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling."""
tokens = text_to_word_sequence(text)
print(tokens)

text = """Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method."""

text_to_word_sequence(text, split= ".", filters="!.\n")

"""#**Gensim**"""

from gensim.utils import tokenize
text = """There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling."""
tokens = list(tokenize(text))
print(tokens)

